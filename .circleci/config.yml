version: 2
jobs:
  deploy:
    docker:
      - image: node:11.10.1

    working_directory: ~/repo

    steps:
      - checkout

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "package.json" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run: yarn install

      - save_cache:
          paths:
            - node_modules
          key: v1-dependencies-{{ checksum "package.json" }}

      # run tests!
      - run: yarn test

      - run: yarn build

      - run: yarn deploy
  # Run Lighthouse against staging deployment -- anonymous user hitting
  # the app's home page
  lighthouse:
    # Number of parallel Lighthouse runs against this url. Why more than one?
    # Some perf metrics vary across runs based on backend flakiness, etc, and
    # this way we can extract a best score or median across runs.
    parallelism: 1

    # Sadly there is currently no official lighthouse docker image. But
    # it is on their radar github.com/GoogleChrome/lighthouse/issues/3715
    docker:
      - image: kporras07/lighthouse-ci

    steps:
      - checkout

      - run:
          name: Run lighthouse against staging deployment

          # Set TEST_URL as an environment variable so that our custom config
          # file (see below) can know about it
          environment:
            TEST_URL: https://lighthouse-circleci.netlify.com/

          # - Extract max JS bundle size, as well as a regular expression that
          #   identifies our main JS bundle, from their definitions in `package.json`.
          #   Store them in environment variables so that our custom confg
          #   can reference them.
          #
          # - Invoke `lighthouse`, passing in a custom config file via --config-path.
          #   The config file is how we include our custom audit.
          #
          # - The container is defined to run as user `chrome` (side note, can't run
          #   Chrome browser as root!) We store the Lighthouse reports in user
          #   `chrome`'s home directory via the --output-path.
          #
          # - Also via --output-path, the reports will have in their file
          #   name `anonymous` (as opposed to authenticated) and a checksum
          #   of a unique value from the container where they were executed
          #
          # - We use --output to get reports in html, so people can view them,
          #   AND json, so we can extract scores from them programmatically.
          command: |
            lighthouse $TEST_URL \
              --port=9222 \
              --chrome-flags=\"--headless\" \
              --output-path=/home/chrome/reports/anonymous-"$(echo -n $CIRCLE_SHELL_ENV | md5sum | awk '{print $1}')" \
              --output=json \
              --output=html
      # Save the reports just generated in a place where the _next job_ in the
      # workflow can snag them all and do analysis on them.
      - persist_to_workspace:
          root: /home/chrome
          paths:
            - reports

workflows:
  version: 2
  deployAndAudit:
    jobs:
      - deploy
      - audit:
          requires:
            - build
